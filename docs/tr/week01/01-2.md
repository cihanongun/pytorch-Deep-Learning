---
lang-ref: ch.01-2
lecturer: Yann LeCun
lang: tr
title: CNN'lerin Evrimi ve Kullanım Alanları ve Neden Derin Öğrenme?
authors: Marina Zavalina, Peeyush Jain, Adrian Pearl, Davida Kollmar
date: 27 Jan 2020
translation-date: 04 Sep 2020
translator: cihanongun
---


<!-- ## [Evolution of CNNs](https://www.youtube.com/watch?v=0bMe_vCZo30&t=2965s) -->
## [CNN'lerin evrimi](https://www.youtube.com/watch?v=0bMe_vCZo30&t=2965s)

<!--
In animal brains, neurons react to edges that are at particular orientations. Groups of neurons that react to the same orientations are replicated over all of the visual field.

Fukushima (1982) built a neural net (NN) that worked the same way as the brain, based on two concepts. First, neurons are replicated across the visual field. Second, there are complex cells that pool the information from simple cells (orientation-selective units). As a result, the shift of the picture will change the activation of simple cells, but will not influence the integrated activation of the complex cell (convolutional pooling).

LeCun (1990) used backprop to train a CNN to recognize handwritten digits. There is a demo from 1992 where the algorithm recognizes the digits of any style. Doing character/pattern recognition using a model that is trained end-to-end was new at that time. Previously, people had used feature extractors with a supervised model on top.

These new CNN systems could recognize multiple characters in the image at the same time. To do it, people used a small input window for a CNN and swiped it over the whole image. If it activated, it meant there was a particular character present.

Later, this idea was applied to faces/people detection and semantic segmentation (pixel-wise classification). Examples include Hadsell (2009) and Farabet (2012). This eventually became popular in industry, used in autonomous driving applications such as lane tracking.

Special types of hardware to train CNN were a hot topic in the 1980s, then the interest dropped, and now it has become popular again.

The deep learning (though the term was not used at that time) revolution started in 2010-2013. Researchers focused on inventing algorithms that could help train large CNNs faster. Krizhevsky (2012) came up with AlexNet, which was a much larger CNN than those used before, and trained it on ImageNet (1.3 million samples) using GPUs. After running for a couple of weeks AlexNet beat the performance of the best competing systems by a large margin -- a 25.8% *vs.* 16.4% top-5 error rate.

After seeing AlexNet's success, the computer vision (CV) community was convinced that CNNs work. While all papers from 2011-2012 that mentioned CNNs had been rejected, since 2016 most accepted CV papers use CNNs.

Over the years, the number of layers used has been increasing: LeNet -- 7, AlexNet -- 12, VGG -- 19, ResNet -- 50. However, there is a trade-off between the number of operations needed to compute the output, the size of the model, and its accuracy. Thus, a popular topic now is how to compress the networks to make the computations faster.
-->

Hayvan beyinlerinde nöronlar belirli yönlerdeki kenarlara tepki verirler. Aynı yönlere tepki veren nöron grupları bütün görüş alanında tekrarlanır.

Fukushima (1982) iki konsepte dayalı olarak, beyinle aynı şekilde çalışan bir sinir ağı (*neural network* - NN) inşa etti. İlki, nöronlar görüş alanı boyunca tekrarlanır. İkincisi, basit hücrelerden bilgi süzen karmaşık hücreler vardır (yön seçici birimler). Sonuç olarak, resmin kaydırılması basit hücrelerin aktivasyonunu değiştirecek ama karmaşık hücrelerin aktivasyonunu etkilemeyecektir (evrişimsel örnekleme - *convolutional pooling*)

LeCun (1990) el yazısı rakamları tanımak için kullandığı bir CNN'i geri yayılım (*backprop*) ile eğitti. 1992 yılında bir demoda algoritmanın her stilden rakamı tanıyabildiği görülebilmektedir. Uçtan uca eğitilen bir model ile karakter/örüntü tanıma o yıllarda yeni bir şeydi. Ondan önce insanlar denetimli bir model ile öznitelik çıkarıcıları kullanıyorlardı.

Bu yeni CNN sistemleri bir resimde aynı anda birden fazla karakteri tanıyabiliyordu. Bunu yapmak için insanlar CNN'de küçük bir girdi penceresini bütün resmin üzerinde kaydırıyorlardı. Aktifleştiğinde orada bir karakter olduğu anlamına geliyordu.

Daha sonra bu fikir yüz/insan tespitine ve anlamsal bölümlemeye (piksel temelli sınıflandırma) uyarlandı. Bu konuda Hadsell (2009) ve Farabet (2012) gibi örnekler mevcuttur. Sonunda bu endüstride popüler oldu ve şerit takibi gibi otonom sürüş uygulamalarında kullanıldı.

1980'lerde CNN eğitmek için özel donanımlar popüler bir konuydu, sonrasında ilgi azaldı fakat şimdi yeniden popüler bir konu.

Derin Öğrenme (bu terim o zamanlar kullanılmıyordu) devrimi 2010-2013 yılları arasında başladı. Araştırmacılar büyük CNN'leri daha hızlı eğitmeye yardımcı olacak algoritmaları keşfetmeye odaklandılar. Krizhevsky (2012) daha önceki CNN'lerden çok daha büyük olan AlexNet'i yarattı ve onu ImageNet (1.3 milyon resim) ile GPU (ekran kartı) kullanarak eğitti. Bir kaç haftalık bir çalışma süresinden sonra AlexNet en iyi rakiplerini büyük farkla alt etti -- ilk 5 (*top-5*) hata oranını 25.8%'ten 16.4%'e düşürdü.

AlexNet'in başarısını gördükten sonra bilgisayarlı görü (*computer vision - CV*) alanındakiler CNN'lerin işe yaradığına ikna oldu. 2011-2012 yıllarında CNN'lerden bahseden tüm makaleler reddedilirken, 2016'dan beri CV alanında kabul edilen makalelerin büyük çoğunluğu CNN kullananlardır.

Yıllar geçtikçe kullanılan katman sayısı da arttı: LeNet -- 7, AlexNet -- 12, VGG -- 19, ResNet -- 50. Fakat burada sonucun hesaplanması için gereken işlem sayısı, modelin büyüklüğü ve doğruluk oranı arasında bir maliyet hesabı vardır. Bu sebepten şu anda popüler olan bir konu da işlemlerin daha hızlı yapılması için modellerin nasıl sıkıştırılabileceğidir.


<!-- ## [Deep Learning and Feature Extraction](https://www.youtube.com/watch?v=0bMe_vCZo30&t=3955s) -->
## [Derin Öğrenme ve Öznitelik Çıkarımı](https://www.youtube.com/watch?v=0bMe_vCZo30&t=3955s)

<!--
Multilayer networks are successful because they exploit the compositional structure of natural data. In compositional hierarchy, combinations of objects at one layer in the hierarchy form the objects at the next layer. If we mimic this hierarchy as multiple layers and let the network learn the appropriate combination of features, we get what is called Deep Learning architecture. Thus, Deep Learning networks are hierarchical in nature.

Deep learning architectures have led to an incredible progress in computer vision tasks ranging from identifying and generating accurate masks around the objects to identifying spatial properties of an object. Mask-RCNN and RetinaNet architectures mainly led to this improvement.

Mask RCNNs have found their use in segmenting individual objects, *i.e.* creating masks for each object in an image. The input and output are both images. The architecture can also be used to do instance segmentation, *i.e.* identifying different objects of the same type in an image. Detectron, a Facebook AI Research (FAIR) software system, implements all these state-of-the-art object detection algorithms and is open source.

Some of the practical applications of CNNs are powering autonomous driving and analysing medical images.

Although the science and mathematics behind deep learning is fairly understood, there are still some interesting questions that require more research. These questions include: Why do architectures with multiple layers perform better, given that we can approximate any function with two layers? Why do CNNs work well with natural data such as speech, images, and text? How are we able to optimize non-convex functions so well? Why do over-parametrised architectures work?

Feature extraction consists of expanding the representational dimension such that the expanded features are more likely to be linearly separable; data points in higher dimensional space are more likely to be linearly separable due to the increase in the number of possible separating planes.

Earlier machine learning practitioners relied on high quality, hand crafted, and task specific features to build artificial intelligence models, but with the advent of Deep Learning, the models are able to extract the generic features automatically. Some common approaches used in feature extraction algorithms are highlighted below:

- Space tiling
- Random Projections
- Polynomial Classifier (feature cross-products)
- Radial basis functions
- Kernel Machines

Because of the compositional nature of data, learned features have a hierarchy of representations with increasing level of abstractions. For example:

-  Images - At the most granular level, images can be thought of as pixels. Combination of pixels constitute edges which when combined forms textons (multi-edge shapes). Textons form motifs and motifs form parts of the image. By combining these parts together we get the final image.
-  Text - Similarly, there is an inherent hierarchy in textual data. Characters form words, when we combine words together we get word-groups, then clauses, then by combining clauses we get sentences. Sentences finally tell us what story is being conveyed.
-  Speech - In speech, samples compose bands, which compose sounds, which compose phones, then phonemes, then whole words, then sentences, thus showing a clear hierarchy in representation.
-->

Çok katmanlı ağlar doğal verinin bileşik yapısından faydalandığı için başarılı olur. Bileşik hiyerarşide, bir katmandaki nesnelerin kombinasyonu bir sonraki katmandaki nesneleri oluşturur. Eğer bu hiyerarşiyi çok katmanlı olarak taklit edersek ve ağın uygun kombinasyonları öğrenmesini sağlarsak, Derin Öğrenme denilen mimariyi elde etmiş oluruz. Bu sebepten Derin Öğrenme ağları doğası gereği hiyerarşik yapıdadır.

Derin öğrenme mimarileri nesnelerin çevrelerini belirten maskeleri tanımak, üretmek ve nesnelerin uzaysal özelliklerini belirlemek gibi bilgisayarlı görü alanındaki problemlerde olağanüstü ilerlemeler sağlamışlardır. Bu ilerleme özellikle Mask-RCNN ve RetinaNet mimarileri sayesinde olmuştur.

Mask-RCNN'ler farklı nesnelerin bölümlenmesi (bir resimdeki her nesne için maske oluşturmak) için kullanılmaktadır. Hem girdi hem de çıktı resim formatındadır. Mimari aynı zamanda örnek bölümleme (bir resimde aynı tipteki farklı nesnelerin tanınması) için de kullanılabilir. Bir Facebook AI Research (FAIR) yazılım sistemi olan Detectron bütün bu güncel en iyi nesne tespiti algoritmalarını barındırır ve açık kaynaklıdır.

CNN'ler otonom sürüş ve medikal görüntülerin analizi gibi konularda kullanılmaktadır.

Derin Öğrenmenin temelinde yatan bilim ve matematik genel olarak anlaşılmış olsa da hala daha fazla araştırma gerektiren ilgi çekici sorular vardır. Bu sorulara şu örnekleri verebiliriz: Herhangi bir fonksiyona yakınlaşmak için iki katmanın yeterli olduğunu biliyoruz, neden daha fazla katmanlı mimariler daha iyi sonuçlar veriyor? Neden CNN'ler konuşma, resim ve metin gibi doğal verilerle iyi çalışıyor? Nasıl dışbükey olmayan fonskiyonları bu kadar iyi optimize edebiliyoruz? Neden gereğinden fazla parametreli mimariler çalışabiliyor?

Öznitelik çıkarma (*feature extraction*) genişletilmiş özniteliklerin daha fazla doğrusal ayrılabilir olması için gösterim boyutunun genişletilmesinden oluşur. Daha üst uzaydaki veri noktaları, daha fazla olası ayırıcı düzleme imkan tanıdığı için doğrusal ayrılabilir olması daha muhtemeldir.

Önceden makine öğrenmesi çalışanları yapay zeka modelleri için yüksek kaliteli, el ile çıkarılmış ve amaca özgü özniteliklere ihtiyaç duyarlardı. Derin Öğrenmenin ortaya çıkmasıyla modeller artık genel öznitelikleri otomatik olarak çıkarabiliyorlar. Öznitelik çıkarımı algoritmalarında yaygın olarak kullanılan bazı yaklaşımlar aşağıdaki gibidir:

- Uzay döşeme
- Rastgele İzdüşüm
- Polinomsal Sınıflandırma (özniteliklerin çapraz çarpımları)
- Radyal tabanlı fonksiyonlar
- Kernel Makineleri

Doğal verinin bileşik yapısından dolayı, öğrenilen öznitelikler her seviyede artan bir soyutlama ile bir gösterim hiyerarşisine sahiplerdir. Örnek:

-  Resimler - En küçük seviyede, resimler pikseller bütünü olarak düşünülebilir. Pikseller birleşip kenarları, kenarlar da birleşip çok kenarlı şekilleri (texton) oluştururlar. Bu şekiller motifleri, motifler de resmin parçalarını oluşturur. Bu parçalar da birleşerek nihai resmi oluşturur.
-  Metin - Benzer şekilde metin verisinde de doğal bir hiyerarşi vardır. Harfler kelimeleri, kelimeler kelime gruplarını, tümceleri ve sonuç olarak cümleleri oluşturur. Cümleler ise anlatılan hikayeye şekil verir.
-  Konuşma - Konuşmada örneklemler dalgaları, dalgalar sırayla sesleri, fonları, fonemleri, kelimeleri ve cümleleri oluşturarak bir gösterim hiyerarşisi sergilerler.

<!-- ## [Learning representations](https://www.youtube.com/watch?v=0bMe_vCZo30&t=4767s) -->
## [Gösterimlerin öğrenilmesi](https://www.youtube.com/watch?v=0bMe_vCZo30&t=4767s)
<!--
There are those who dismiss Deep Learning: if we can approximate any function with 2 layers, why have more?

For example: SVMs find a separating hyperplane "in the span of the data", meaning predictions are based on comparisons to training examples. SVMs are essentially a very simplistic 2 layer neural net, where the first layer defines "templates" and the second layer is a linear classifier. The problem with 2 layer fallacy is that the complexity and size of the middle layer is exponential in $N$ (to do well with a difficult task, need LOTS of templates). But if you expand the number of layers to $\log(N)$, the layers become linear in $N$. There is a trade-off between time and space.

An analogy is designing a circuit to compute a boolean function with no more than two layers of gates -- we can compute **any boolean function** this way! But, the complexity and resources of the first layer (number of gates) quickly becomes infeasible for complex functions.

What is "deep"?

- An SVM isn't deep because it only has two layers
- A classification tree isn't deep because every layer analyses the same (raw) features
- A deep network has several layers and uses them to build a **hierarchy of features of increasing complexity**

How can models learn representations (good features)?

Manifold hypothesis: natural data lives in a low-dimensional manifold. Set of possible images is essentially infinite, set of "natural" images is a tiny subset. For example: for an image of a person, the set of possible images is on the order of magnitude of the number of face muscles they can move (degrees of freedom) ~ 50. An ideal (and unrealistic) feature extractor represents all the factors of variation (each of the muscles, lighting, *etc.*).

Q&A from the end of lecture:

- For the face example, could some other dimensionality reduction technique (*i.e.* PCA) extract these features?
  - Answer: would only work if the manifold surface is a hyperplane, which it is not
 -->
 Derin Öğrenmeyi kabul etmeyenler de vardır : eğer herhangi bir fonksiyona 2 katman ile yakınlaşabiliyorsak neden daha fazla kullanalım ki?
 
 Örneğin: SVM'ler (*Destek Vektör Makineleri*) verinin kapsamında ayırıcı bir hiper düzlem bulurlar, yani üretilen tahminler eğitim örnekleriyle kıyaslamaya dayalıdır. SVM'ler aslında çok basit 2 katmanlı bir sinir ağıdır, ilk katman "şablonları" (*templates*) tanımlar, ikinci katman ise bir doğrusal sınıflandırıcıdır. 2 katmanın yeteceği düşüncesinin yanlışlığı şudur; orta katmanın karmaşıklığı ve boyutu $N$ üsteldir (zorlu bir problemi çözmek için çok fazla sayıda şablon gerekir). Fakat katman sayısını $\log(N)$ arttırırsanız, katmanlar $N$ doğrusal olur. Burada zaman ve uzay arasında bir maliyet hesabı vardır.
 
Bunu bir mantıksal (*boolean*) fonksiyon hesaplamak için sadece 2 katmanlı mantık kapıları kullanan bir elektronik devre kartı dizayn etmeye benzetebiliriz, **herhangi bir mantıksal fonksiyonu** bu yöntemle hesaplayabiliriz. Fakat karmaşık fonksiyonlar için ilk katmanın karmaşıklığı ve gerekli kaynaklar (kapı sayısı) kısa sürede imkansız boyutlara ulaşır.


"Derin" ne demek?

- SVM derin değildir çünkü sadece 2 katmanı vardır.
- Sınıflandırma ağacı derin değildir çünkü her katman aynı (ham) öznitelikleri analiz eder.
- Derin ağın çok sayıda katmanı vardır ve bu katmanları **giderek artan karmaşıklıktaki özniteliklerin hiyerarşisi**ni oluşturmak için kullanır.

Modeller gösterimleri nasıl öğrenir (iyi öznitelikler)?

Manifold (*çok katlı*) hipotezi: doğal veri az boyutlu bir manifold'dadır. Olası bütün resimler kümesi sonsuzdur, "doğal" resimler kümesi ise küçük bir alt kümedir. Örneğin bir insan resmi için olası bütün resimler kümesi, hareket ettirilebilir yüz kaslarının sayısının (serbestlik derecesi ~ 50) üstel büyüklüğü kadardır. İdeal bir (gerçekçi olmayan) öznitelik çıkarıcı bütün değişim öğelerinin (her kas, ışık, vb.) gösterimini yapar.

Ders sonunda Soru&Cevap:
- Yüz örneği için, başka bir boyut azaltma (*dimension reduction*) tekniği (PCA) bu öznitelikleri çıkartabilir mi?
  - Cevap: Sadece manifold yüzeyi hiper düzlem olsaydı işe yarardı, bu durumda değil
